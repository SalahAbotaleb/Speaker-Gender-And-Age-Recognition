{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d21e0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf27d72",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4ee6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 48000  # Sample rate\n",
    "N_MFCC = 20  # MFCC features\n",
    "GMM_COMPONENTS = 32  # For faster training; use 512 in production\n",
    "IVECTOR_DIM = 100  # i-vector size (total variability space)\n",
    "MFCC_DIR = r\"trials\\features\\mfcc_extra_stats_2000_n\" # r\"trials\\features\\48k_mfcc_extra_hfcc_extra\"\n",
    "Y_DIR = r\"trials\\features\\mfcc_extra_stats_2000_n\" # r\"trials\\features\\48k_mfcc_extra_hfcc_extra\"\n",
    "\n",
    "filtered_metadata_path = os.path.join(\".\", \"data\", \"filtered_data_labeled.tsv\")\n",
    "audio_dir = os.path.join(\".\", \"data\", \"filtered_clips\")\n",
    "df = pd.read_csv(filtered_metadata_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300272e",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47eae2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mfccs(df):\n",
    "    \"\"\"\n",
    "    Generator that yields MFCC features for each audio file in the dataframe.\n",
    "    \"\"\"\n",
    "    mfccs_list = []\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading MFCCs\"):\n",
    "        filename = row['path']\n",
    "        filepath = os.path.join(audio_dir, filename)\n",
    "        audio, sr = librosa.load(filepath, sr=SR)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC).T\n",
    "        mfccs_list.append(mfccs)\n",
    "    return mfccs_list\n",
    "\n",
    "def train_ubm(mfccs_list, n_components=GMM_COMPONENTS):\n",
    "    all_feats = np.vstack(mfccs_list)\n",
    "    ubm = GaussianMixture(n_components=n_components, covariance_type='diag', max_iter=100)\n",
    "    ubm.fit(all_feats)\n",
    "    return ubm\n",
    "\n",
    "def train_t_matrix(mfccs_list, ubm, R=100, n_iter=5):\n",
    "    K, D = ubm.means_.shape\n",
    "    T = np.random.randn(K * D, R).astype(np.float32) * 0.1\n",
    "\n",
    "    # Precompute per-utterance stats\n",
    "    stats = []\n",
    "    for mfccs in tqdm(mfccs_list, desc=\"Computing Baum-Welch stats\"):\n",
    "        N, F = compute_bw_stats(mfccs, ubm)\n",
    "        # Convert to float32 to reduce memory usage\n",
    "        stats.append((N.astype(np.float32), F.astype(np.float32)))\n",
    "\n",
    "    for iteration in range(n_iter):\n",
    "        T_num = np.zeros((K * D, R), dtype=np.float32)\n",
    "        T_den = np.zeros((R, R), dtype=np.float32)\n",
    "\n",
    "        for N, F in tqdm(stats, desc=f\"T-matrix EM iter {iteration+1}\"):\n",
    "            # Centered stats\n",
    "            S = np.zeros((K, D), dtype=np.float32)\n",
    "            for k in range(K):\n",
    "                S[k] = F[k] - N[k] * ubm.means_[k]\n",
    "            S = S.flatten()\n",
    "\n",
    "            # Inverse sigma (diagonal covs)\n",
    "            sigma = ubm.covariances_.reshape(K, D).flatten().astype(np.float32) + 1e-6\n",
    "            inv_sigma = 1. / sigma\n",
    "            T_invSigma = T.T * inv_sigma[None, :]\n",
    "\n",
    "            # E-step: compute posterior for i-vector w\n",
    "            cov_w = np.linalg.inv(T_invSigma @ T + np.eye(R, dtype=np.float32))\n",
    "            mean_w = cov_w @ (T_invSigma @ S)\n",
    "\n",
    "            # M-step accumulators - memory-efficient implementation\n",
    "            # Avoid creating large temporary arrays with np.outer\n",
    "            for r in range(R):\n",
    "                T_num[:, r] += S * mean_w[r]\n",
    "            \n",
    "            T_den += N.sum() * (np.outer(mean_w, mean_w) + cov_w)\n",
    "\n",
    "        # Update T\n",
    "        T = T_num @ np.linalg.inv(T_den)\n",
    "\n",
    "    return T, stats\n",
    "\n",
    "def compute_bw_stats(mfccs, ubm):\n",
    "    if mfccs.ndim == 1:\n",
    "        mfccs = mfccs.reshape(1, -1)\n",
    "    responsibilities = ubm.predict_proba(mfccs)\n",
    "    N = np.sum(responsibilities, axis=0)  # [K]\n",
    "    F = np.dot(responsibilities.T, mfccs)  # [K, D]\n",
    "    return N, F\n",
    "\n",
    "def extract_ivec(N, F, ubm, T):\n",
    "    \"\"\"\n",
    "    Extract an i-vector using full per-component covariances.\n",
    "    N: [K] - zero order stats\n",
    "    F: [K, D] - first order stats\n",
    "    T: [K*D, R] - total variability matrix\n",
    "    \"\"\"\n",
    "    K, D = ubm.means_.shape\n",
    "    R = T.shape[1]\n",
    "\n",
    "    # Flattened UBM means and covariances\n",
    "    m = ubm.means_.flatten()\n",
    "    covs = ubm.covariances_.reshape(K, D)  # still diagonal, but per component\n",
    "    T_blocks = T  # shape: (K*D, R)\n",
    "\n",
    "    # Compute centered stats\n",
    "    F_dev = (F - N[:, None] * ubm.means_).flatten()  # (K*D,)\n",
    "\n",
    "    # Construct precision matrix (inverse of covariance)\n",
    "    inv_sigma = 1.0 / covs.flatten()  # (K*D,)\n",
    "    T_transpose_Sigma_inv = T_blocks.T * inv_sigma[None, :]  # (R, K*D)\n",
    "\n",
    "    # Compute posterior covariance of i-vector (R x R)\n",
    "    cov_i = np.linalg.inv(T_transpose_Sigma_inv @ T_blocks + np.eye(R))\n",
    "\n",
    "    # Compute posterior mean of i-vector (R,)\n",
    "    mean_i = cov_i @ (T_transpose_Sigma_inv @ F_dev)\n",
    "\n",
    "    return mean_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fd696",
   "metadata": {},
   "source": [
    "Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b1d6cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Splits...\n",
      "Loading MFCCs...\n",
      "Loading labels...\n",
      "Training UBM...\n",
      "Training T-matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Baum-Welch stats: 100%|██████████| 7200/7200 [00:09<00:00, 759.56it/s]\n",
      "T-matrix EM iter 1: 100%|██████████| 7200/7200 [05:31<00:00, 21.71it/s]\n",
      "T-matrix EM iter 2: 100%|██████████| 7200/7200 [05:25<00:00, 22.12it/s]\n",
      "T-matrix EM iter 3: 100%|██████████| 7200/7200 [05:25<00:00, 22.14it/s]\n",
      "T-matrix EM iter 4: 100%|██████████| 7200/7200 [05:26<00:00, 22.07it/s]\n",
      "T-matrix EM iter 5: 100%|██████████| 7200/7200 [05:35<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting i-vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting i-vectors: 100%|██████████| 7200/7200 [03:05<00:00, 38.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Loading test MFCCs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing test stats: 100%|██████████| 800/800 [00:01<00:00, 675.73it/s]\n",
      "Extracting test i-vectors: 100%|██████████| 800/800 [00:22<00:00, 35.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating classifier...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       M_20s       0.62      0.69      0.65       200\n",
      "       F_20s       0.71      0.81      0.75       200\n",
      "       M_50s       0.69      0.60      0.64       200\n",
      "       F_50s       0.76      0.65      0.70       200\n",
      "\n",
      "    accuracy                           0.69       800\n",
      "   macro avg       0.69      0.69      0.69       800\n",
      "weighted avg       0.69      0.69      0.69       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main():\n",
    "    print(\"Loading Splits...\")\n",
    "    # df_train, df_temp = train_test_split(df, test_size=0.10, random_state=42)\n",
    "    # df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(\"Loading MFCCs...\")\n",
    "    # Stratified split to train and test\n",
    "    mfccs_all = joblib.load(os.path.join(MFCC_DIR, \"X.joblib\"))\n",
    "\n",
    "    print(\"Loading labels...\")\n",
    "    # y_train = df_train['label'].values\n",
    "    # y_val = df_val['label'].values\n",
    "    # y_test = df_test['label'].values\n",
    "    y_all = joblib.load(os.path.join(Y_DIR, \"y.joblib\"))\n",
    "\n",
    "    mfccs_train, mfccs_val = train_test_split(mfccs_all, test_size=0.1, random_state=42, stratify=y_all) \n",
    "    y_train, y_val = train_test_split(y_all, test_size=0.1, random_state=42, stratify=y_all)\n",
    "\n",
    "    print(\"Training UBM...\")\n",
    "    ubm = train_ubm(mfccs_train)\n",
    "\n",
    "    print(\"Training T-matrix...\")\n",
    "    T, stats = train_t_matrix(mfccs_train, ubm, R=IVECTOR_DIM, n_iter=5)\n",
    "\n",
    "    print(\"Extracting i-vectors...\")\n",
    "    ivecs = []\n",
    "    for (N, F) in tqdm(stats, desc=\"Extracting i-vectors\"):\n",
    "        ivec = extract_ivec(N, F, ubm, T)\n",
    "        ivecs.append(ivec)\n",
    "    ivecs = np.vstack(ivecs)\n",
    "\n",
    "    print(\"Training classifier...\")\n",
    "    # Train classifier using the extracted i-vectors\n",
    "    clf = KNeighborsClassifier(n_neighbors=4)\n",
    "    clf.fit(ivecs, y_train)\n",
    "\n",
    "    # For test data, need to extract i-vectors first\n",
    "    print(\"Loading test MFCCs...\")\n",
    "    # mfcc_val = load_mfccs(df_val)\n",
    "\n",
    "    val_stats = []\n",
    "    for mfcc in tqdm(mfccs_val, desc=\"Computing test stats\"):\n",
    "        N, F = compute_bw_stats(mfcc, ubm)\n",
    "        val_stats.append((N, F))\n",
    "\n",
    "    val_ivecs = []\n",
    "    for (N, F) in tqdm(val_stats, desc=\"Extracting test i-vectors\"):\n",
    "        ivec = extract_ivec(N, F, ubm, T)\n",
    "        val_ivecs.append(ivec)\n",
    "    val_ivecs = np.vstack(val_ivecs)\n",
    "\n",
    "    # Evaluate on i-vectors\n",
    "    print(\"Evaluating classifier...\")\n",
    "    preds = clf.predict(val_ivecs)\n",
    "    print(classification_report(y_val, preds, target_names=[\"M_20s\", \"F_20s\", \"M_50s\", \"F_50s\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
