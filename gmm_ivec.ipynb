{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21e0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf27d72",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4ee6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 48000  # Sample rate\n",
    "N_MFCC = 20  # MFCC features\n",
    "GMM_COMPONENTS = 32  # For faster training; use 512 in production\n",
    "IVECTOR_DIM = 100  # i-vector size (total variability space)\n",
    "MFCC_DIR = r\"trials\\features\\48k_mfcc_extra_hfcc_extra\"\n",
    "Y_DIR = r\"trials\\features\\48k_mfcc_extra_hfcc_extra\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300272e",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47eae2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ubm(mfccs_list, n_components=GMM_COMPONENTS):\n",
    "    all_feats = np.vstack(mfccs_list)\n",
    "    ubm = GaussianMixture(n_components=n_components, covariance_type='diag', max_iter=100)\n",
    "    ubm.fit(all_feats)\n",
    "    return ubm\n",
    "\n",
    "def train_t_matrix(mfccs_list, ubm, R=100, n_iter=5):\n",
    "    K, D = ubm.means_.shape\n",
    "    T = np.random.randn(K * D, R) * 0.1\n",
    "\n",
    "    # Precompute per-utterance stats\n",
    "    stats = []\n",
    "    for mfccs in tqdm(mfccs_list, desc=\"Computing Baum-Welch stats\"):\n",
    "        N, F = compute_bw_stats(mfccs, ubm)\n",
    "        stats.append((N, F))\n",
    "\n",
    "    for iteration in range(n_iter):\n",
    "        T_num = np.zeros((K * D, R))\n",
    "        T_den = np.zeros((R, R))\n",
    "\n",
    "        for N, F in tqdm(stats, desc=f\"T-matrix EM iter {iteration+1}\"):\n",
    "            # Centered stats\n",
    "            S = np.zeros((K, D))\n",
    "            for k in range(K):\n",
    "                S[k] = F[k] - N[k] * ubm.means_[k]\n",
    "            S = S.flatten()\n",
    "\n",
    "            # Inverse sigma (diagonal covs)\n",
    "            sigma = ubm.covariances_.reshape(K, D).flatten() + 1e-6\n",
    "            inv_sigma = 1. / sigma\n",
    "            T_invSigma = T.T * inv_sigma[None, :]\n",
    "\n",
    "            # E-step: compute posterior for i-vector w\n",
    "            cov_w = np.linalg.inv(T_invSigma @ T + np.eye(R))\n",
    "            mean_w = cov_w @ (T_invSigma @ S)\n",
    "\n",
    "            # M-step accumulators\n",
    "            T_num += np.outer(S, mean_w)\n",
    "            T_den += N.sum() * (np.outer(mean_w, mean_w) + cov_w)\n",
    "\n",
    "        # Update T\n",
    "        T = T_num @ np.linalg.inv(T_den)\n",
    "\n",
    "    return T, stats\n",
    "\n",
    "def compute_bw_stats(mfccs, ubm):\n",
    "    if mfccs.ndim == 1:\n",
    "        mfccs = mfccs.reshape(1, -1)\n",
    "    responsibilities = ubm.predict_proba(mfccs)\n",
    "    N = np.sum(responsibilities, axis=0)  # [K]\n",
    "    F = np.dot(responsibilities.T, mfccs)  # [K, D]\n",
    "    return N, F\n",
    "\n",
    "def extract_ivec(N, F, ubm, T):\n",
    "    \"\"\"\n",
    "    Extract an i-vector using full per-component covariances.\n",
    "    N: [K] - zero order stats\n",
    "    F: [K, D] - first order stats\n",
    "    T: [K*D, R] - total variability matrix\n",
    "    \"\"\"\n",
    "    K, D = ubm.means_.shape\n",
    "    R = T.shape[1]\n",
    "\n",
    "    # Flattened UBM means and covariances\n",
    "    m = ubm.means_.flatten()\n",
    "    covs = ubm.covariances_.reshape(K, D)  # still diagonal, but per component\n",
    "    T_blocks = T  # shape: (K*D, R)\n",
    "\n",
    "    # Compute centered stats\n",
    "    F_dev = (F - N[:, None] * ubm.means_).flatten()  # (K*D,)\n",
    "\n",
    "    # Construct precision matrix (inverse of covariance)\n",
    "    inv_sigma = 1.0 / covs.flatten()  # (K*D,)\n",
    "    T_transpose_Sigma_inv = T_blocks.T * inv_sigma[None, :]  # (R, K*D)\n",
    "\n",
    "    # Compute posterior covariance of i-vector (R x R)\n",
    "    cov_i = np.linalg.inv(T_transpose_Sigma_inv @ T_blocks + np.eye(R))\n",
    "\n",
    "    # Compute posterior mean of i-vector (R,)\n",
    "    mean_i = cov_i @ (T_transpose_Sigma_inv @ F_dev)\n",
    "\n",
    "    return mean_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fd696",
   "metadata": {},
   "source": [
    "Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d6cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features...\n",
      "Loaded 181880 samples.\n",
      "Loading labels...\n",
      "Training UBM...\n",
      "Training T-matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Baum-Welch stats: 100%|██████████| 172786/172786 [01:28<00:00, 1945.02it/s]\n",
      "T-matrix EM iter 1: 100%|██████████| 172786/172786 [1:05:26<00:00, 44.00it/s]\n",
      "T-matrix EM iter 2:  12%|█▏        | 21097/172786 [05:15<2:28:39, 17.01it/s]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Loading features...\")\n",
    "    mfcc_train = joblib.load(os.path.join(MFCC_DIR, 'X_train.joblib'))[:,:150]\n",
    "    mfcc_test = joblib.load(os.path.join(MFCC_DIR, 'X_test.joblib'))[:,:150]\n",
    "    print(f\"Loaded {len(mfcc_train) + len(mfcc_test)} samples.\")\n",
    "\n",
    "    print(\"Loading labels...\")\n",
    "    y_train = joblib.load(os.path.join(Y_DIR, 'y_train.joblib'))\n",
    "    y_test = joblib.load(os.path.join(Y_DIR, 'y_test.joblib'))\n",
    "\n",
    "    print(\"Training UBM...\")\n",
    "    ubm = train_ubm(mfcc_train)\n",
    "\n",
    "    print(\"Training T-matrix...\")\n",
    "    T, stats = train_t_matrix(mfcc_train, ubm, R=IVECTOR_DIM, n_iter=5)\n",
    "\n",
    "    print(\"Extracting i-vectors...\")\n",
    "    ivecs = []\n",
    "    for (N, F) in tqdm(stats, desc=\"Extracting i-vectors\"):\n",
    "        ivec = extract_ivec(N, F, ubm, T)\n",
    "        ivecs.append(ivec)\n",
    "    ivecs = np.vstack(ivecs)\n",
    "\n",
    "    print(\"Training classifier...\")\n",
    "    # Train classifier using the extracted i-vectors\n",
    "    clf = KNeighborsClassifier(n_neighbors=4)\n",
    "    clf.fit(ivecs, y_train)\n",
    "\n",
    "    # For test data, need to extract i-vectors first\n",
    "    test_stats = []\n",
    "    for mfcc in tqdm(mfcc_test, desc=\"Computing test stats\"):\n",
    "        N, F = compute_bw_stats(mfcc, ubm)\n",
    "        test_stats.append((N, F))\n",
    "\n",
    "    test_ivecs = []\n",
    "    for (N, F) in tqdm(test_stats, desc=\"Extracting test i-vectors\"):\n",
    "        ivec = extract_ivec(N, F, ubm, T)\n",
    "        test_ivecs.append(ivec)\n",
    "    test_ivecs = np.vstack(test_ivecs)\n",
    "\n",
    "    # Evaluate on i-vectors\n",
    "    print(\"Evaluating classifier...\")\n",
    "    preds = clf.predict(test_ivecs)\n",
    "    print(classification_report(y_test, preds, target_names=[\"M_20s\", \"F_20s\", \"M_50s\", \"F_50s\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
